# -*- coding: utf-8 -*-
"""reddit_mental_health.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/georgiepayne/reddit_mental_health_sentimentanalysis/blob/master/reddit_mental_health.ipynb
"""

from datasets import load_dataset
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import string
import nltk
from nltk.corpus import stopwords  # To help get a list of stopwords to exclude
nltk.download('stopwords') # The word bank we references to delete stopwords
from collections import Counter # To help count the frequent words
from nltk.stem.porter import PorterStemmer  # To help stem words
from spellchecker import SpellChecker  # To help correct spelling
import re  # To help remove urls

ds_load = load_dataset("solomonk/reddit_mental_health_posts")
print(type(ds_load['train']))

df = pd.DataFrame(ds_load['train'])

# Replace the body with a title any time it gets deleted or removed

# Strip any whitespace from the `body` column
body_stripped = df['body'].str.strip()

# Check if the `body` contains `[deleted]` or `[removed]`
is_deleted = body_stripped.isin(['[deleted]', '[removed]'])

# Replace the entry in body with the entry in title
df.loc[is_deleted, 'body'] = ""

df.head(10)

# Helper functions that the preprocess function calls

def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

# Helper functions that the preprocess function calls
def remove_stopwords(text):

  # Get the stopwords from from the english language
  # This data is found in the stopwords libarary
  STOPWORDS = set(stopwords.words('english'))

  # Split the string into and array of words
  words = str(text).split()

  filtered = []  # To keep track of the filtered words

  # Loop through the words array
  for word in words:

    # Append any words not in the STOPWORDS array
    filtered.append(word) if word not in STOPWORDS else None

  # Make the final string by adding spaces between all stop words
  cleaned_string = " ".join(filtered)
  return cleaned_string

def remove_frequent_and_rare(data, top_n=10, min_frequency=10):
  counter = Counter()  # make a counter

  # Count words from both 'body' and 'title' columns
  for col in ["body", "title"]:
    # get entrues as strings
    data[col] = data[col].astype(str)

    # update counter for each word
    for text in data[col]:
      for word in text.split():
        counter[word] +=1

  # Get the top N most frequent words
  most_frequent_words = {word for word, _ in counter.most_common(top_n)}

  # Get the rare words (show up less than min_frequency)
  rare_words = {word for word, count in counter.items() if count < min_frequency}

  # Combine frequent and rare words to be filtered out
  words_to_filter = most_frequent_words.union(rare_words)

  # Filter the 'body' and 'title' columns
  for col in ["body", "title"]:
    filtered_column = []
    for text in data[col]:
      words = text.split()
      filtered_words = [word for word in words if word not in words_to_filter]
      filtered_column.append(" ".join(filtered_words))

    # Update the DataFrame column
    data[col] = filtered_column

  return data


def stem_words(text):
  stemmer = PorterStemmer()
  words = text.split()

  # Store the filtered words in the array
  filtered = []
  # Go through the text
  for word in words:
    # And stem each words
    filtered.append(stemmer.stem(word))

  cleaned_string = " ".join(filtered)
  return cleaned_string

def expand_abbreviations(text):

  # Turn a the text file containing abbreviations into a data frame
  path = "abbreviations.txt"
  df = pd.read_csv(path, sep="=", names=["abbreviation", "expanded"], header=None)

  abbreviations_dictionary = {}

  # I work with arrays better so I'm converting it into one
  data_array = df[["abbreviation", "expanded"]].values

  # Fill up the dictionary
  for row in data_array:
    abbreviation = row[0]
    expanded = row[1]
    abbreviations_dictionary[abbreviation] = expanded

  expanded_text = []

  words = text.split()

  # Go through the text
  for word in words:
    # Check if the abbreviation matches anything in the dictionary
    if word in abbreviations_dictionary:
      expanded_text.append( abbreviations_dictionary[word] )
    else:
      expanded_text.append(word)

  cleaned_string = " ".join(expanded_text)
  return cleaned_string


def remove_urls(text):
  url_pattern = re.compile(r'https?://\S+|www\.\S+')
  return url_pattern.sub(r'', text)

# Preprocessing function the cleans the the input data
def preprocess(data):
  # Set everything to lowercase
  data["body"] = data["body"].str.lower()
  data["title"] = data["title"].str.lower()

  # Remove all punctuation
  data["body"] = data["body"].apply(lambda text: remove_punctuation(text) if text is not None else text)
  data["title"] = data["title"].apply(lambda text: remove_punctuation(text) if text is not None else text)

  # remove stop words
  data["body"] = data["body"].apply(lambda text: remove_stopwords(text) if text is not None else text)
  data["title"] = data["title"].apply(lambda text: remove_stopwords(text) if text is not None else text)

  # remove frequent words and rare words
  data = remove_frequent_and_rare(data)

  # apply stemming
  data["body"] = data["body"].apply(lambda text: stem_words(text) if text is not None else text)
  data["title"] = data["title"].apply(lambda text: stem_words(text) if text is not None else text)

  # expand abbreviations
  data["body"] = data["body"].apply(lambda text: expand_abbreviations(text) if text is not None else text)
  data["title"] = data["title"].apply(lambda text: expand_abbreviations(text) if text is not None else text)

  # remove urls
  data["body"] = data["body"].apply(lambda text: remove_urls(text) if text is not None else text)
  data["title"] = data["title"].apply(lambda text: remove_urls(text) if text is not None else text)

  return data


preprocessed_df = preprocess(df)
print(preprocessed_df.head())

preprocessed_df.to_csv('preprocessed_data.csv', index=False)

X = df[['body', 'title']]   # Input the model uses to make a prediction
y = df['subreddit']         # Metal health issue the model should predict

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# bag of words - sklearn count vectorizer